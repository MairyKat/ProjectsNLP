# -*- coding: utf-8 -*-
"""OpenAI_Whisper_ASR_Demo_M.Katsani.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LjUvlDtdXi_4PZFf067KwzVg5nEpA34s

# OpenAI's Whisper Speech Recognition Model

## Install the Whisper Code
"""

! pip install git+https://github.com/openai/whisper.git -q

"""## Load the ML Model"""

import whisper

model = whisper.load_model("base")

"""## Check GPU


"""

model.device

"""## Download Test Audio Files

This repository has a couple of pre-recorded MP3s to run through the transcribe function. You can listen to them with the audio widgets displayed below.
"""

!git clone https://github.com/petewarden/openai-whisper-webapp

from IPython.display import Audio
Audio("/content/openai-whisper-webapp/mary.mp3")

from IPython.display import Audio
Audio("/content/openai-whisper-webapp/daisy_HAL_9000.mp3")

"""## Define the Transcribe Function

This function takes an audio file path as an input and returns the recognized text (and logs what it thinks the language is).
"""

def transcribe(audio):

    # load audio and pad/trim it to fit 30 seconds
    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    # make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # detect the spoken language
    _, probs = model.detect_language(mel)
    print(f"Detected language: {max(probs, key=probs.get)}")

    # decode the audio
    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    return result.text

"""## Test with Pre-Recorded Audio

"""

easy_text = transcribe("/content/openai-whisper-webapp/mary.mp3")
print(easy_text)

hard_text = transcribe("/content/openai-whisper-webapp/daisy_HAL_9000.mp3")
print(hard_text)

"""## Install the Web UI Toolkit

We'll be using gradio to provide the widgets we need to do audio recording.
"""

! pip install gradio -q

import gradio as gr
import time

"""## Web Interface"""

gr.Interface(
    title = 'OpenAI Whisper ASR Gradio Web UI',
    fn=transcribe,
    inputs=[
        gr.Audio(type="filepath")
    ],
    outputs=[
        "textbox"
    ],
    live=True).launch()

#Los audio de la práctica anterior no pueden ser reconocidos en la Web Interface propablemente debido a su tamaño tan pequeño, por eso utilizamos la función que hemos creado previamente.

#Bata grabación 1
model = whisper.load_model("medium.en", device="cuda")
transcription = model.transcribe("/content/openai-whisper-webapp/bata_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Bata grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/bata_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Caca grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/caca_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Caca grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/caca_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Cara grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/cara_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Cara grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/cara_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Cata grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/cata_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Cata grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/cata_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Paca grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/paca_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Paca grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/paca_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Para grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/para_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Para grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/para_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#Pata grabación 1
transcription = model.transcribe("/content/openai-whisper-webapp/pata_mk_1.wav", task="transcribe", language="es")
print(transcription["text"])

#Pata grabación 2
transcription = model.transcribe("/content/openai-whisper-webapp/pata_mk_2.wav", task="transcribe", language="es")
print(transcription["text"])

#En general el modelo reconoce bien las palabras pronunciadas.
#Con la excepción de la palabras "para" que en la transcripción ha dado "pada".
#En la palabra "cata" que en una de las transcripciones ha dado "katha".
#En la palabra "paka" que en una de las transcripciones ha dado "baka".

beam_size=5
best_of=5
temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
decode_options = dict(language="en", best_of=best_of, beam_size=beam_size,
temperature=temperature)
transcribe_options = dict(task="transcribe", **decode_options)
transcription = model.transcribe("/content/openai-whisper-webapp/Obi-Wan-says-Hello.m4a",task="transcribe", language="en")
print(transcription["text"])

#4.Grabe una frase compleja, por ejemplo: "Cuando cuentas cuentos, cuenta cuántos
#cuentos cuentas porque si no cuentas cuántos cuentos cuentas nunca sabrás cuántos
#cuentos cuentas tú."
#5. Pruebe el reconocimiento con las opciones por defecto y con opciones modificadas,
#compruebe las diferencias

beam_size=5
best_of=5
temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
decode_options = dict(language="es", best_of=best_of, beam_size=beam_size,
temperature=temperature)
transcribe_options = dict(task="transcribe", **decode_options)
transcription = model.transcribe("/content/openai-whisper-webapp/ejercicio_5.m4a",task="transcribe", language="en")
print(transcription["text"])

#Parace que el programa distingue con éxito la mayoría de las palabras que son muy parecidas, sin embargo falla en las palabras que son únicas dentro de la frase como "nunca sabrás".

#Cambiamos los parametros como se sugiere en el ejercicio y el resultado parece ser el mismo.

beam_size=5
best_of=None
temperature=0.0
decode_options = dict(language="es", best_of=best_of, beam_size=beam_size,
temperature=temperature)
transcription = model.transcribe("/content/openai-whisper-webapp/ejercicio_5.m4a",task="transcribe", language="en")
print(transcription["text"])

#6. Modifique las opciones de forma similar sobre un archivo que haya fallado para ver si cambia.

beam_size = 5
best_of = 5
temperature = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0)
decode_options = dict(language="es", best_of=best_of, beam_size=beam_size, temperature=temperature)
transcribe_options = dict(task="transcribe", **decode_options)
transcription = model.transcribe("/content/openai-whisper-webapp/ejercicio_5.m4a", **transcribe_options)
print(transcription["text"])

#Cambiando los parametros de la manera anterior se consigue un resultado mejor ya que ahora identifica correctamente las palabra "nunca sabrás".